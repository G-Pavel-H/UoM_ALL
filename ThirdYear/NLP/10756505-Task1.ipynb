{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Distributional semantics\n",
    "\n",
    "\n",
    "The task is to use different vector representations to estimate the cosine similarity between two terms. I will use methods a) and b):\n",
    "\n",
    "a) a sparse representation BoW with tf*idf;\n",
    "\n",
    "\n",
    "b) a dense static representation word2vec;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preprocessing the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the path to the dataset\n",
    "dataset_path = \"./data/\"\n",
    "\n",
    "# Reading the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(dataset_path + \"Training-dataset.csv\")\n",
    "\n",
    "# Merging the 'title' and 'plot_synopsis' columns to create a unified 'text' column\n",
    "df['text'] = df['title'] + ' ' + df['plot_synopsis']\n",
    "\n",
    "# Selecting relevant columns for training data, including 'text' and various genre labels\n",
    "training_data = df[['text', 'comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing method to be applied on the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a set of English stopwords and additional custom stopwords\n",
    "stop_words = set(stopwords.words('english') + ['reuter', '\\x03'])\n",
    "\n",
    "# Initializing a lemmatizer for text processing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Uncomment the following line to enable stemming using Porter Stemmer\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "def preprocessor(text: str):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by performing the following steps:\n",
    "    1. Converting text to lowercase.\n",
    "    2. Removing punctuation using a translation table.\n",
    "    3. Replacing digits with the placeholder 'num'.\n",
    "    4. Filtering out stopwords.\n",
    "    5. Lemmatizing each word.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing punctuation using translation table\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(table)\n",
    "\n",
    "    # Replacing digits with 'num'\n",
    "    text = re.sub(r'\\d+', 'num', text)\n",
    "\n",
    "    # Filtering out stopwords\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "    # Lemmatizing each word\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Uncomment the following line to enable stemming using Porter Stemmer\n",
    "    # text = [stemmer.stem(word) for word in text]\n",
    "\n",
    "    return \" \".join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>comedy</th>\n",
       "      <th>cult</th>\n",
       "      <th>flashback</th>\n",
       "      <th>historical</th>\n",
       "      <th>murder</th>\n",
       "      <th>revenge</th>\n",
       "      <th>romantic</th>\n",
       "      <th>scifi</th>\n",
       "      <th>violence</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Si wang ta After a recent amount of challenges...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>si wang ta recent amount challenge billy lo br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shattered Vengeance In the crime-ridden city o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>shattered vengeance crimeridden city tremont r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L'esorciccio Lankester Merrin is a veteran Cat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>lesorciccio lankester merrin veteran catholic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Serendipity Through Seasons \"Serendipity Throu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>serendipity season serendipity season heartwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Liability Young and naive 19-year-old slac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>liability young naive numyearold slacker adam ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  comedy  cult  flashback  \\\n",
       "0  Si wang ta After a recent amount of challenges...       0     0          0   \n",
       "1  Shattered Vengeance In the crime-ridden city o...       0     0          0   \n",
       "2  L'esorciccio Lankester Merrin is a veteran Cat...       0     1          0   \n",
       "3  Serendipity Through Seasons \"Serendipity Throu...       0     0          0   \n",
       "4  The Liability Young and naive 19-year-old slac...       0     0          1   \n",
       "\n",
       "   historical  murder  revenge  romantic  scifi  violence  \\\n",
       "0           0       1        1         0      0         1   \n",
       "1           0       1        1         1      0         1   \n",
       "2           0       0        0         0      0         0   \n",
       "3           0       0        0         1      0         0   \n",
       "4           0       0        0         0      0         0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  si wang ta recent amount challenge billy lo br...  \n",
       "1  shattered vengeance crimeridden city tremont r...  \n",
       "2  lesorciccio lankester merrin veteran catholic ...  \n",
       "3  serendipity season serendipity season heartwar...  \n",
       "4  liability young naive numyearold slacker adam ...  "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the preprocessor function to the 'text' column and creating a new 'preprocessed_text' column\n",
    "training_data['preprocessed_text'] = training_data['text'].apply(preprocessor)\n",
    "\n",
    "# Displaying the first few rows of the updated training_data DataFrame\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method A: Sparse representation BoW with tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "The TfidfVectorizer class in scikit-learn (sklearn) is a part of the feature extraction module and is used to convert a collection of raw documents to a matrix of TF-IDF features. It has multiple parameters which include:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **max_df (default=1.0)**: When building the vocabulary, terms that have a document frequency strictly higher than the given threshold are ignored. This can be a float in the range [0.0, 1.0] or an integer.\n",
    "\n",
    "\n",
    "* **min_df (default=1)**: Similar to max_df, but terms with a document frequency lower than the given threshold are ignored.\n",
    "\n",
    "\n",
    "* **max_features (default=None)**: If not None, limits the number of features (terms) in the vocabulary to the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "\n",
    "* **norm (default='l2')**: This parameter specifies the normalization method for the term vectors. Options include 'l1', 'l2', or None.\n",
    "\n",
    "\n",
    "* **'ngram_range'**: This parameter in the TfidfVectorizer class specifies the range of n-grams to include in the feature matrix\n",
    "\n",
    "In this scenario I have obtained the parameter values through manual tuning meaning I have tried multiple combinations on the validation dataset and recorded the accuracy metric provided by the evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a TF-IDF vectorizer with specified parameters\n",
    "# Using ngram_range to implement bi-gram functionality as required\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=3, ngram_range=(1, 2), norm='l1')\n",
    "\n",
    "# Transforming the preprocessed text data using the TF-IDF vectorizer\n",
    "tfidf = vectorizer.fit_transform(training_data['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing is the main method which calculates the cosine simlarity of two terms. First, I obtain the vectors of the given terms from the already existing vectorized form while at the same time checking if the term is in the vocabulary. If it is not in the vocabulary, its vector is set to ones instead of zeros because I was getting better results with such approach. Afterwards, the cosine similarity is calculated using the obtained vector values through the cosine_similarity method from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_A(term1, term2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between the TF-IDF vectors of two terms in the dataset.\n",
    "\n",
    "    Args:\n",
    "    - term1 (str): First term for similarity comparison.\n",
    "    - term2 (str): Second term for similarity comparison.\n",
    "\n",
    "    Returns:\n",
    "    - float: Cosine similarity between the TF-IDF vectors of the two terms.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the index of each term in the TF-IDF matrix vocabulary\n",
    "    term1_index = vectorizer.vocabulary_.get(term1, -1)\n",
    "    term2_index = vectorizer.vocabulary_.get(term2, -1)\n",
    "\n",
    "    # If a term is not present in the vocabulary, create a dummy TF-IDF vector with all ones\n",
    "    if term1_index == -1:\n",
    "        tfidf_vector_for_term_1 = np.ones((1, training_data.shape[0]))\n",
    "    else:\n",
    "        tfidf_vector_for_term_1 = tfidf[:, term1_index]\n",
    "\n",
    "    if term2_index == -1:\n",
    "        tfidf_vector_for_term_2 = np.ones((1, training_data.shape[0]))\n",
    "    else:\n",
    "        tfidf_vector_for_term_2 = tfidf[:, term2_index]\n",
    "\n",
    "    # Calculate cosine similarity between the TF-IDF vectors of the two terms\n",
    "    similarity = cosine_similarity(tfidf_vector_for_term_1.reshape(1, -1), tfidf_vector_for_term_2.reshape(1, -1))[0][0]\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results generation for given input data\n",
    "\n",
    "The follwoing method will be used to obtain results and write them to a file for given input data in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_output(input_data, data_path, method, val_file=True):\n",
    "    \"\"\"\n",
    "    Generates output CSV files containing similarity scores based on the specified method.\n",
    "\n",
    "    Args:\n",
    "    - input_data (str): Name of the input CSV file containing term pairs.\n",
    "    - data_path (str): Path to the directory containing the input files.\n",
    "    - method (str): Method to be used ('A' or 'B') for similarity calculation.\n",
    "    - val_file (bool): Indicates whether the input file is a validation file.\n",
    "\n",
    "    Returns:\n",
    "    - None: Outputs a CSV file with similarity scores based on the chosen method.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine column names and output file name based on method and validation status\n",
    "    if val_file and method == 'A':\n",
    "        col_names = ['index', 'term1', 'term2', 'score']\n",
    "        output_name = '10756505-Task1-method-a-validation.csv'\n",
    "    elif val_file and method == 'B':\n",
    "        col_names = ['index', 'term1', 'term2', 'score']\n",
    "        output_name = '10756505-Task1-method-b-validation.csv'\n",
    "    elif not val_file and method == 'A':\n",
    "        col_names = ['index', 'term1', 'term2']\n",
    "        output_name = '10756505-Task1-method-a.csv'\n",
    "    elif not val_file and method == 'B':\n",
    "        col_names = ['index', 'term1', 'term2']\n",
    "        output_name = '10756505-Task1-method-b.csv'\n",
    "    else:\n",
    "        print(\"Invalid method arg\")\n",
    "        return\n",
    "\n",
    "    # Read the input CSV file with specified column names\n",
    "    validation_file = pd.read_csv(data_path + input_data, names=col_names)\n",
    "    all_similarity_vals = []\n",
    "\n",
    "    # Iterate through rows and calculate similarity scores based on the chosen method\n",
    "    for index, row in validation_file.iterrows():\n",
    "        if method == 'A':\n",
    "            similarity_val = sim_A(row['term1'], row['term2'])\n",
    "        elif method == 'B':\n",
    "            similarity_val = sim_B(row['term1'], row['term2'])\n",
    "        else:\n",
    "            print(\"Invalid method arg\")\n",
    "            return\n",
    "\n",
    "        all_similarity_vals.append(similarity_val)\n",
    "\n",
    "    # Add the calculated similarity scores to the DataFrame\n",
    "    validation_file['prediction_score'] = all_similarity_vals\n",
    "\n",
    "    # Create a DataFrame with 'index' and 'prediction_score' columns\n",
    "    prediction_df = validation_file[['index', 'prediction_score']]\n",
    "\n",
    "    # Save the prediction DataFrame to a CSV file\n",
    "    prediction_df.to_csv(data_path + output_name, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_output(\"Task-1-validation-dataset.csv\", dataset_path, 'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_output(\"Task-1-test-dataset1.csv\", dataset_path, 'A', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method B: Dense static representation with Word2vec\n",
    "\n",
    "### Gensim Model\n",
    "\n",
    "For this method I am going to utilize gensim library word2vec class. I am going to use the same data which was already preprocessed for Method A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim.models.Word2Vec is a popular implementation of the Word2Vec algorithm in the Gensim library, which is widely used for word embedding. Word embedding is a technique that represents words as vectors in a continuous vector space, capturing semantic relationships between words. Some key parameters of the Word2Vec model:\n",
    "\n",
    "* **sentences**: This parameter takes a list of sentences as input, where each sentence is represented as a list of words. The model learns vector representations for words based on the context in which they appear in these sentences.\n",
    "* **vector_size**: The size of the word vectors. This parameter sets the dimensionality of the word vectors. Common values are in the range of 100 to 300.\n",
    "* **window**: The maximum distance between the current and predicted word within a sentence. It defines the context window for learning word representations. Words outside this window are not considered as context words.\n",
    "* **min_count**: Ignores all words with a total frequency lower than this. This helps to filter out rare words that may not have sufficient context for meaningful vector representations.\n",
    "\n",
    "Below parameter values manually fine tuned baed on the accuracy of the evaluation script on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Word2Vec model using gensim\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=[t.split() for t in training_data['preprocessed_text'].to_list()],\n",
    "    vector_size=100,  # Size of the word vectors\n",
    "    window=10,  # Maximum distance between the current and predicted word within a sentence\n",
    "    min_count=3  # Ignores all words with a total frequency lower than this\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using similar logic for finding the similarity to the one for method A. I am setting the vetor with ones if the term is an OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_B(term1, term2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between the Word2Vec vectors of two terms in the dataset.\n",
    "\n",
    "    Args:\n",
    "    - term1 (str): First term for similarity comparison.\n",
    "    - term2 (str): Second term for similarity comparison.\n",
    "\n",
    "    Returns:\n",
    "    - float: Cosine similarity between the Word2Vec vectors of the two terms.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Get vectors for the two terms from the Word2Vec model\n",
    "        vector1 = model.wv[term1].reshape(1, -1)\n",
    "        vector2 = model.wv[term2].reshape(1, -1)\n",
    "\n",
    "    except KeyError as e:\n",
    "        # Handle the case where one or both terms are not in the vocabulary\n",
    "        vector1 = np.ones((1, model.vector_size))\n",
    "        vector2 = np.ones((1, model.vector_size))\n",
    "\n",
    "    # Calculate cosine similarity using sklearn's cosine_similarity function\n",
    "    similarity_score = cosine_similarity(vector1, vector2)[0, 0]\n",
    "\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_output(\"Task-1-validation-dataset.csv\", dataset_path, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_output(\"Task-1-test-dataset1.csv\", dataset_path, 'B', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given more time and computational resources it would havebeen possible to further fine-tune parameters of Tfidfvectorizers and gensim.Word2vec which would lead to higher accuracy scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
