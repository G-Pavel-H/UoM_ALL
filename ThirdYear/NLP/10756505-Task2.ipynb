{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text classification\n",
    "\n",
    "\n",
    "The task is to implement, train and evaluate a multi-label text classifier that assigns document-level labels to each document in a corpus. I will use methods a) and b):\n",
    "\n",
    "a) Developing a “traditional” classification method with SVM;\n",
    "\n",
    "\n",
    "b) Developing a “traditional” deep learning method with  bi-directional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing method to be applied on the dataframe (the same from Task 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a set of English stopwords and additional custom stopwords\n",
    "stop_words = set(stopwords.words('english') + ['reuter', '\\x03'])\n",
    "\n",
    "# Initializing a lemmatizer for text processing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Uncomment the following line to enable stemming using Porter Stemmer\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "def preprocessor(text: str):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by performing the following steps:\n",
    "    1. Converting text to lowercase.\n",
    "    2. Removing punctuation using a translation table.\n",
    "    3. Replacing digits with the placeholder 'num'.\n",
    "    4. Filtering out stopwords.\n",
    "    5. Lemmatizing each word.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing punctuation using translation table\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(table)\n",
    "\n",
    "    # Replacing digits with 'num'\n",
    "    text = re.sub(r'\\d+', 'num', text)\n",
    "\n",
    "    # Filtering out stopwords\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "    # Lemmatizing each word\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Uncomment the following line to enable stemming using Porter Stemmer\n",
    "    # text = [stemmer.stem(word) for word in text]\n",
    "\n",
    "    return \" \".join(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading, analysing and pre-processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'comedy' plots: 1262\n",
      "Number of 'cult' plots: 1801\n",
      "Number of 'flashback' plots: 1994\n",
      "Number of 'historical' plots: 186\n",
      "Number of 'murder' plots: 4019\n",
      "Number of 'revenge' plots: 1657\n",
      "Number of 'romantic' plots: 2006\n",
      "Number of 'scifi' plots: 204\n",
      "Number of 'violence' plots: 3064\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>comedy</th>\n",
       "      <th>cult</th>\n",
       "      <th>flashback</th>\n",
       "      <th>historical</th>\n",
       "      <th>murder</th>\n",
       "      <th>revenge</th>\n",
       "      <th>romantic</th>\n",
       "      <th>scifi</th>\n",
       "      <th>violence</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Si wang ta After a recent amount of challenges...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>si wang ta recent amount challenge billy lo br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shattered Vengeance In the crime-ridden city o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>shattered vengeance crimeridden city tremont r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L'esorciccio Lankester Merrin is a veteran Cat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>lesorciccio lankester merrin veteran catholic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Serendipity Through Seasons \"Serendipity Throu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>serendipity season serendipity season heartwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Liability Young and naive 19-year-old slac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>liability young naive numyearold slacker adam ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  comedy  cult  flashback  \\\n",
       "0  Si wang ta After a recent amount of challenges...       0     0          0   \n",
       "1  Shattered Vengeance In the crime-ridden city o...       0     0          0   \n",
       "2  L'esorciccio Lankester Merrin is a veteran Cat...       0     1          0   \n",
       "3  Serendipity Through Seasons \"Serendipity Throu...       0     0          0   \n",
       "4  The Liability Young and naive 19-year-old slac...       0     0          1   \n",
       "\n",
       "   historical  murder  revenge  romantic  scifi  violence  \\\n",
       "0           0       1        1         0      0         1   \n",
       "1           0       1        1         1      0         1   \n",
       "2           0       0        0         0      0         0   \n",
       "3           0       0        0         1      0         0   \n",
       "4           0       0        0         0      0         0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  si wang ta recent amount challenge billy lo br...  \n",
       "1  shattered vengeance crimeridden city tremont r...  \n",
       "2  lesorciccio lankester merrin veteran catholic ...  \n",
       "3  serendipity season serendipity season heartwar...  \n",
       "4  liability young naive numyearold slacker adam ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the path to the dataset\n",
    "dataset_path = \"./data/\"\n",
    "\n",
    "# Reading the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(dataset_path + \"Training-dataset.csv\")\n",
    "\n",
    "# Creating separate DataFrames for each genre based on binary labels\n",
    "comedy_df = df.loc[df[\"comedy\"] == 1]\n",
    "cult_df = df.loc[df[\"cult\"] == 1]\n",
    "flashback_df = df.loc[df[\"flashback\"] == 1]\n",
    "historical_df = df.loc[df[\"historical\"] == 1]\n",
    "murder_df = df.loc[df[\"murder\"] == 1]\n",
    "revenge_df = df.loc[df[\"revenge\"] == 1]\n",
    "romantic_df = df.loc[df[\"romantic\"] == 1]\n",
    "scifi_df = df.loc[df[\"scifi\"] == 1]\n",
    "violence_df = df.loc[df[\"violence\"] == 1]\n",
    "\n",
    "# Creating a list of DataFrames for each genre\n",
    "sep_label_df = [comedy_df, cult_df, flashback_df,\n",
    "                historical_df,\n",
    "                murder_df,\n",
    "                revenge_df,\n",
    "                romantic_df,\n",
    "                scifi_df,\n",
    "                violence_df\n",
    "                ]\n",
    "\n",
    "# Displaying the number of plots for each genre\n",
    "col_val = 3\n",
    "for i in sep_label_df:\n",
    "    print(f\"Number of '{i.columns[col_val]}' plots: {i.shape[0]}\")\n",
    "    col_val += 1\n",
    "\n",
    "# Combining the title and plot synopsis into a single 'text' column\n",
    "df['text'] = df['title'] + ' ' + df['plot_synopsis']\n",
    "\n",
    "# Selecting relevant columns for training data\n",
    "training_data = df[['text', 'comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n",
    "\n",
    "# Applying a preprocessing function to the 'text' column\n",
    "training_data['preprocessed_text'] = training_data['text'].apply(preprocessor)\n",
    "\n",
    "# Displaying the first few rows of the preprocessed training data\n",
    "training_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I have realized how unbalanced my data is because for example out of all the documents only 186 are labelled as 'historical' which would not be the best amount to learn 'historical' label. For this reason, I have decided to split my data by 80/20 for train and test not by random subsampling but making sure that I get 80% of all the label specific documents. Code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select a specified percentage of rows for training\n",
    "def training_rows(data, perc=0.8):\n",
    "    \"\"\"\n",
    "    Selects a specified percentage of rows for training from the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame, the input data for training\n",
    "    - perc: float, the percentage of rows to be used for training (default is 0.8)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame, subset of the input data for training\n",
    "    \"\"\"\n",
    "    return data.head(int(len(data) * perc))\n",
    "\n",
    "# Function to select rows for testing based on the training set\n",
    "def testing_rows(data, train):\n",
    "    \"\"\"\n",
    "    Selects the rows for testing that are not included in the training set.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame, the complete dataset\n",
    "    - train: DataFrame, the training set\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame, subset of the input data for testing\n",
    "    \"\"\"\n",
    "    return data.iloc[len(train):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty sets to store unique indices for training and testing data\n",
    "train_id_set = []\n",
    "test_id_set = []\n",
    "\n",
    "# Iterating over genre-specific DataFrames to split them into training and testing sets\n",
    "for i in sep_label_df:\n",
    "    # Selecting rows for training and testing for each genre\n",
    "    i_train = training_rows(i)\n",
    "    i_test = testing_rows(i, i_train)\n",
    "    \n",
    "    # Extending the sets with unique indices for both training and testing\n",
    "    train_id_set.extend(i_train.index.unique())\n",
    "    test_id_set.extend(i_test.index.unique())\n",
    "\n",
    "# Converting the lists to sets to ensure uniqueness of indices\n",
    "train_id_set = set(train_id_set)\n",
    "test_id_set = set(test_id_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can divide my available data for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting preprocessed text data for training and testing based on the generated indices\n",
    "X_train = training_data.loc[train_id_set, \"preprocessed_text\"]\n",
    "X_test = training_data.loc[test_id_set, \"preprocessed_text\"]\n",
    "\n",
    "# Extracting binary label data for training and testing based on the generated indices\n",
    "y_train = training_data.loc[train_id_set, ['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n",
    "y_test = training_data.loc[test_id_set, ['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method A: SVM “traditional” classification method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completing this task I will use TfidfVectorizer class from sklearn (used in Task 1) alongside with LinearSVC class and OneVsRestClassifier from sklearn.\n",
    "\n",
    "In scikit-learn, LinearSVC stands for Linear Support Vector Classification, which is a linear classification model. It is based on the liblinear library and is similar to the SVC (Support Vector Classification) with a linear kernel, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in terms of loss functions and penalties.\n",
    "\n",
    "Some important parameters of the LinearSVC class in scikit-learn:\n",
    "\n",
    "* **C** (float, optional, default=1.0):\n",
    "This is the regularization parameter, also known as the penalty parameter. It controls the trade-off between achieving a low training error and a low testing error. A smaller C encourages a larger-margin separating hyperplane, but at the cost of training accuracy. A larger C may result in a smaller-margin hyperplane but better training accuracy.\n",
    "\n",
    "* **loss** (string, optional, default='squared_hinge'):\n",
    "Specifies the loss function. It can take values like 'hinge' or 'squared_hinge'. 'Hinge' is the standard SVM loss (soft-margin) while 'squared_hinge' is the square of the hinge loss.\n",
    "\n",
    "The OneVsRestClassifier in scikit-learn is a meta-estimator that fits one binary classifier per class. It is commonly used for multilabel classification where each class is considered as a separate binary classification task.\n",
    "\n",
    "In order to fine-tune the parameters for LinearSVC and Tfidfvectorizer, I have used sklearn Pipeline and GridSearchCV.\n",
    "\n",
    "A Pipeline in scikit-learn is a way to streamline a lot of the routine processes, making it easier to keep track of the various steps involved in a machine learning workflow. It allows you to assemble several steps that can be cross-validated together while setting different parameters.\n",
    "\n",
    "The primary purpose of a Pipeline is to assemble several steps that can be cross-validated together, while setting different parameters.\n",
    "\n",
    "GridSearchCV is a method for systematically working through multiple combinations of hyperparameters to find the best parameters for a model. It performs an exhaustive search over a specified parameter grid, evaluating the model performance for each combination of parameters using cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__estimator__C': 1, 'classifier__estimator__dual': 'auto', 'tfidf__max_df': 0.8, 'tfidf__min_df': 1, 'tfidf__norm': 'l2'}\n",
      "Best Cross-validated F1 Score: 0.43\n",
      "Test Set Accuracy: 0.23\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules for building the pipeline and performing grid search\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Creating a pipeline with TF-IDF vectorization and a One-vs-Rest classifier using LinearSVC\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', OneVsRestClassifier(LinearSVC()))\n",
    "])\n",
    "\n",
    "# Define the parameter grid to search during grid search\n",
    "param_grid = {\n",
    "    'classifier__estimator__C': [0.1, 0.5, 0.8, 1],\n",
    "    'classifier__estimator__dual': [\"auto\"],\n",
    "    'tfidf__max_df': [0.3, 0.5, 0.8, 1],\n",
    "    'tfidf__min_df': [1, 3, 5, 10],\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation (cv=10) using F1 score for multilabel classification\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='f1_samples', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and their corresponding cross-validated F1 score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Cross-validated F1 Score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set and print the accuracy\n",
    "test_accuracy = best_pipeline.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy: {:.2f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Before using LinearSVC class I have used the normal SVC class and carried out cross validation on that including the kernel parameter for 'linear', 'poly' and 'rbf'. I have found out that the best parameter for kernel always is 'linear' and for this reason I directly started using LinearSVC which is much more robust and efficient than SVC class. Additionally, the best parameters are found using the f1 score as it is what is going to be tested later by the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards I am creating my instances using the best parameters GridSearchCV found but now I will not split the Training dataset but will use all of it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_out_A(input_data, data_path, val_file=True):\n",
    "    \"\"\"\n",
    "    Generates classification results using Method A for a given input data and saves the results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data: str, the name of the input CSV file containing plot and title data\n",
    "    - data_path: str, the path to the data directory\n",
    "    - val_file: bool, indicating whether the input file is a validation file (default is True)\n",
    "\n",
    "    Returns:\n",
    "    - None, saves the results to a CSV file based on the specified output name\n",
    "    \"\"\"\n",
    "\n",
    "    # Reading the input data\n",
    "    validation_file = pd.read_csv(data_path + input_data)\n",
    "\n",
    "    # Combining title and plot synopsis into a single 'text' column\n",
    "    validation_file['text'] = validation_file['title'] + ' ' + validation_file['plot_synopsis']\n",
    "\n",
    "    # Applying preprocessing to the 'text' column\n",
    "    validation_file['preprocessed_text'] = validation_file['text'].apply(preprocessor)\n",
    "\n",
    "    # Selecting text data for training and testing\n",
    "    X_validation_train = training_data[\"preprocessed_text\"]\n",
    "    X_validation_test = validation_file['preprocessed_text']\n",
    "\n",
    "    # Selecting binary label data for training\n",
    "    y_validation_train = training_data[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n",
    "\n",
    "    # Creating a TF-IDF vectorizer with best parameters from grid search\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=grid_search.best_params_['tfidf__max_df'],\n",
    "                                       min_df=grid_search.best_params_['tfidf__min_df'],\n",
    "                                       norm=grid_search.best_params_['tfidf__norm'])\n",
    "\n",
    "    # Transforming text data to TF-IDF vectors for both training and testing\n",
    "    X_validation_train_tfidf = tfidf_vectorizer.fit_transform(X_validation_train)\n",
    "    X_validation_test_tfidf = tfidf_vectorizer.transform(X_validation_test)\n",
    "\n",
    "    # Creating a linear support vector classifier (SVM) with best parameter from grid search\n",
    "    svm_classifier = OneVsRestClassifier(LinearSVC(C=grid_search.best_params_['classifier__estimator__C']))\n",
    "    svm_classifier.fit(X_validation_train_tfidf, y_validation_train)\n",
    "\n",
    "    # Predicting genre labels for the validation data\n",
    "    y_validation_pred = svm_classifier.predict(X_validation_test_tfidf)\n",
    "\n",
    "    # Creating a DataFrame with the predicted labels\n",
    "    df_nd_array = pd.DataFrame(y_validation_pred, columns=['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence'])\n",
    "\n",
    "    # Concatenating the ID column from the validation file with the predicted labels\n",
    "    result_df = pd.concat([validation_file['ID'], df_nd_array], axis=1)\n",
    "\n",
    "    # Determining the output file name based on whether the input file is a validation file\n",
    "    if val_file:\n",
    "        output_name = '10756505-Task2-method-a-validation.csv'\n",
    "    else:\n",
    "        output_name = '10756505-Task2-method-a.csv'\n",
    "\n",
    "    # Saving the results to a CSV file without headers and index\n",
    "    result_df.to_csv(data_path + output_name, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_out_A(\"Task-2-validation-dataset.csv\", dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_out_A(\"Task-2-test-dataset1.csv\", dataset_path, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method B:  Bi-LSTM “traditional” deep learning method\n",
    "\n",
    "### Tensorflow.keras\n",
    "\n",
    "For this method I am going to utilize tensorflow.keras LSequential model with Bidirectional layer and LSTM layer. I am going to use the same data which was already preprocessed for Method A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential** is a linear stack of layers in TensorFlow's Keras API. It allows to create models layer-by-layer in a step-by-step fashion. Each layer has weights that correspond to the layer that follows it. You can add layers using the add() method.\n",
    "\n",
    "**Bidirectional** layer in Keras allows to process the input data in both forward and backward directions.\n",
    "This is often used with recurrent layers like LSTM to capture patterns that depend on the order of the input sequence in both directions.\n",
    "It takes another layer as an argument, and the combined output is the concatenation of the forward and backward layer outputs.\n",
    "\n",
    "**Long Short-Term Memory (LSTM)** is a type of recurrent neural network (RNN) layer in Keras.\n",
    "It is designed to overcome the vanishing gradient problem in standard RNNs, making it more effective in learning and remembering long-term dependencies in sequential data.\n",
    "The LSTM layer has memory cells that can store and retrieve information over long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now train the model using the training dataset and validate it on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data[\"preprocessed_text\"].values\n",
    "y_train = training_data[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes have a lot of hyperparmeters to tune. Due to lack of time and computational resources I was only label to produce the below values. If I had more computational power, I would tune the following main parameters: max_words, max_len, embeddin_dim, epochs and batch_size. The current values are the best results from manual experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_word_count(string_array):\n",
    "    \"\"\"\n",
    "    Calculates the average word count for an array of strings.\n",
    "\n",
    "    Parameters:\n",
    "    - string_array: numpy array or list, containing strings for which the word count will be calculated\n",
    "\n",
    "    Returns:\n",
    "    - float, the average word count across all strings in the array\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to calculate word count for a single string\n",
    "    def word_count(string):\n",
    "        return len(string.split())\n",
    "\n",
    "    # Vectorize the word_count function to apply it to each element of the array\n",
    "    vectorized_word_count = np.vectorize(word_count)\n",
    "\n",
    "    # Apply the vectorized function to the array to obtain an array of word counts\n",
    "    word_counts = vectorized_word_count(string_array)\n",
    "\n",
    "    # Calculate the average word count from the array of word counts\n",
    "    average_word_count = np.mean(word_counts)\n",
    "\n",
    "    return average_word_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created the above method to find the average length of a movie_plot because I have found out that the max_len parameter is the best when it is about the average size of a document in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_out_B(input_data, data_path, val_file=True):\n",
    "    \"\"\"\n",
    "    Generates classification results using Method B for a given input data and saves the results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data: str, the name of the input CSV file containing plot and title data\n",
    "    - data_path: str, the path to the data directory\n",
    "    - val_file: bool, indicating whether the input file is a validation file (default is True)\n",
    "\n",
    "    Returns:\n",
    "    - None, saves the results to a CSV file based on the specified output name\n",
    "    \"\"\"\n",
    "\n",
    "    # Reading the input data\n",
    "    validation_file = pd.read_csv(data_path + input_data)\n",
    "\n",
    "    # Combining title and plot synopsis into a single 'text' column\n",
    "    validation_file['text'] = validation_file['title'] + ' ' + validation_file['plot_synopsis']\n",
    "\n",
    "    # Applying preprocessing to the 'text' column\n",
    "    validation_file['preprocessed_text'] = validation_file['text'].apply(preprocessor)\n",
    "\n",
    "    # Determining the output file name based on whether the input file is a validation file\n",
    "    if val_file:\n",
    "        output_name = '10756505-Task2-method-b-validation.csv'\n",
    "    else:\n",
    "        output_name = '10756505-Task2-method-b.csv'\n",
    "\n",
    "    # Extracting preprocessed text data for testing\n",
    "    X_test = validation_file[\"preprocessed_text\"].values\n",
    "\n",
    "    # Setting parameters for tokenization and padding\n",
    "    max_words = 12000  \n",
    "    max_len = round(average_word_count(X_train))\n",
    "\n",
    "    # Tokenizing and padding the text data\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "    # Defining the LSTM model architecture\n",
    "    embedding_dim = 100 \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(9, activation='sigmoid'))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training the model\n",
    "    epochs = 5  \n",
    "    batch_size = 16  \n",
    "    model.fit(X_train_pad, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "\n",
    "    # Predicting genre labels for the validation data\n",
    "    y_pred = model.predict(X_test_pad)\n",
    "\n",
    "    # Converting predicted probabilities to binary labels\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Creating a DataFrame with the predicted labels\n",
    "    df_nd_array = pd.DataFrame(y_pred_binary, columns=['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence'])\n",
    "\n",
    "    # Concatenating the ID column from the validation file with the predicted labels\n",
    "    result_df = pd.concat([validation_file['ID'], df_nd_array], axis=1)\n",
    "\n",
    "    # Saving the results to a CSV file without headers and index\n",
    "    result_df.to_csv(data_path + output_name, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "465/465 [==============================] - 77s 159ms/step - loss: 0.4651 - accuracy: 0.2561 - val_loss: 0.4443 - val_accuracy: 0.2712\n",
      "Epoch 2/5\n",
      "465/465 [==============================] - 76s 163ms/step - loss: 0.4191 - accuracy: 0.3033 - val_loss: 0.4240 - val_accuracy: 0.2603\n",
      "Epoch 3/5\n",
      "465/465 [==============================] - 74s 159ms/step - loss: 0.3512 - accuracy: 0.3925 - val_loss: 0.4466 - val_accuracy: 0.2603\n",
      "Epoch 4/5\n",
      "465/465 [==============================] - 75s 162ms/step - loss: 0.2800 - accuracy: 0.5036 - val_loss: 0.4975 - val_accuracy: 0.2857\n",
      "Epoch 5/5\n",
      "465/465 [==============================] - 73s 157ms/step - loss: 0.2079 - accuracy: 0.5735 - val_loss: 0.5791 - val_accuracy: 0.2760\n",
      "38/38 [==============================] - 3s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "generate_results_out_B(\"Task-2-validation-dataset.csv\", dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "465/465 [==============================] - 76s 158ms/step - loss: 0.4573 - accuracy: 0.2665 - val_loss: 0.4290 - val_accuracy: 0.2893\n",
      "Epoch 2/5\n",
      "465/465 [==============================] - 74s 159ms/step - loss: 0.4011 - accuracy: 0.3210 - val_loss: 0.4227 - val_accuracy: 0.2797\n",
      "Epoch 3/5\n",
      "465/465 [==============================] - 82s 176ms/step - loss: 0.3279 - accuracy: 0.4256 - val_loss: 0.4524 - val_accuracy: 0.2893\n",
      "Epoch 4/5\n",
      "465/465 [==============================] - 76s 162ms/step - loss: 0.2512 - accuracy: 0.5173 - val_loss: 0.5117 - val_accuracy: 0.2869\n",
      "Epoch 5/5\n",
      "465/465 [==============================] - 71s 154ms/step - loss: 0.1860 - accuracy: 0.5788 - val_loss: 0.5833 - val_accuracy: 0.2978\n",
      "38/38 [==============================] - 3s 57ms/step\n"
     ]
    }
   ],
   "source": [
    "generate_results_out_B(\"Task-2-test-dataset1.csv\", dataset_path, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
